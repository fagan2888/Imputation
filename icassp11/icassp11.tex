% Template for ICASSP-2010 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{url}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{IMPUTATION OF BEAT-ALIGNED FEATURES FOR MUSIC PATTERN LEARNING}
%
% Single address.
% ---------------
%\name{Thierry Bertin-Mahieux\thanks{Thanks to NSERC and some other stuff.}}
%\address{EE dept., Columbia University}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
\twoauthors
  {Thierry Bertin-Mahieux\sthanks{NSERC}, Ron J. Weiss\sthanks{Thanks something}}
	{Columbia University / New York University\\
          LabROSA / MARL\\
	New York, USA}
  {Graham Grindlay and Daniel P.W. Ellis}
	{Columbia University\\
          LabROSA\\
	New York, USA}

\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
Imputation is cool. It is well-defined problem, as opposed to segmentation.
Gives a reasonable benchmark to compare algorithms that claim learning meaningful
patterns on music. Hard to beat benchmarks comparison, for instance linear
prediction. We compare 6 methods on a large dataset.
\end{abstract}
%
\begin{keywords}
Missing data, chroma features, imputation, SIPLCA, codebook learning
\end{keywords}
%
\section{Introduction}
\label{sec:intro}

See abstract.

In this paper: definition of the task, presentation of the algorithms, experimental
results.

\section{RELATED WORK}
\label{sec:relatedwork}

Paris Smaragdis on speech data \cite{Smaragdis2009} and
before for supervised ASR \cite{Morris1998}.
In other fields: \cite{Oba2003}.

Similarity with work on automatic music composition such as \cite{Todd1989,Mozer1994a,Eck2002d} or music
expectation \cite{Hazan2010}, but we do not further consider these tasks in this work.

Learned patterns for music data recently includes: \cite{Bertin-Mahieux2010a,Casey2007,Weiss2010}.

If the mask span enough beats (more than one ``segment''), links with music segmentation.
Music segmentation based on chroma features include \cite{Weiss2010,Levy2008,Mauch2009}.

HMM \cite{Rabiner1989} GHMM library\footnote{GHMM: \url{http://ghmm.org/}}.

\section{TASK DEFINITION}
\label{sec:task}

\begin{figure}[t]
\begin{center}
\includegraphics[width=.9\columnwidth]{basic_example}
\end{center}
\caption{Basic example}
\label{fig:code}
\end{figure}

Beat-chroma features used in \cite{Ellis2007a}.
Note that they are extracted from actual audio signals unlike work on automatic composition
\cite{Eck2002d}. That makes them noisier and more complex, but more realistic and usefull
in practice.

The most general definition is that we remove some pixels from a beat-chroma matrix and
we try to recover them. However, in does not make much sense to miss only some frequencies
at a given beat, for instance all the `G' notes. Therefore, we will only remove full
beats.

The final definition is, given a song and a window of missing beats (one or more),
try to impute the missing beats as close to the original ones as possible.

The reconstruction error (or divergence) measure is not obvious. In this work,
we report the euclidean distance and the symmetric Kullback-Leibler divergence.
A discussion on this choice follows in section \ref{sec:experiments}.


\section{METHODS}
\label{sec:methods}
In this section we present the different methods we use for imputing the missing data.
The ``trivial methods'' that do not require any learning, and simply use the available
data from the rest of the song to make an educated guess. We will later see that this
eudcated guess is remarkably good.

The learned models are of more interest for future research, even though they do not
necessarily perform better at the moment. Learning a model that predicts subsequent
notes or harmonic patterns can be applied to other tasks such as cover recognition,
segmentation, similarity and compression.

\subsection{TRIVIAL METHODS}
\label{ssec:trivialmethods}
Below are four of the methods we evaluate, starting by the name we use in the experiments
to refer to them.
\begin{enumerate}
\item \textbf{Random}: each missing pixel is filled with a pseudo-random number drawn from
a constant $[0,1)$ distribution.
\item \textbf{Random-song}: each missing beat is filled with another beat from the song,
chosen at random.
\item \textbf{Average}: each missing beat is filled with the average of the nearby beats,
the window size of nearby beats being a parameter. Note that we look at both previous and
later beats.
\item \textbf{Knn-song}: we compare a window of beats around the missing one with
all possible windows of the same size in the song. We use the closest one (based on euclidean
distance or KL-divergence) to fill in the missing beat. Note that we look at both
previous and later beats.
\end{enumerate}

\subsection{LEARNED MODELS}
\label{ssec:learnedmodels}

Linear predictor, codebook, SIPLCA.

SIPLCA code available online \cite{Weiss2010}. Code to get Echo Nest features and perform
online clustering available online \cite{Bertin-Mahieux2010a}.

\section{EXPERIMENTS}
\label{sec:experiments}

\subsection{ERROR MEASURES}
\label{ssec:errmeasures}
We measure squared euclidean distance or symmetric Kullback-Leibler (KL) divergence,
both averaged over missing pixels. It is unclear which measure is the beat for features
as the ones we use. In \cite{Sajda2003}, author discuss the two for
a NMF method and show that they depend on the noise model that is assumed.
A poisson noise model (versus a gaussian one) seems to better fit audio perception, 
which result in the use of KL. See also \cite{Fevotte2009} for a comparison on a piano
excerpt.

Note that in the following experiments, the two measures seem to agree.

\subsection{DATA}
\label{ssec:data}
We build beat-align chromagrams using the beat tracking and chroma features as
returned by the Echo Nest API \cite{EchoNest}. Our results are on the Cowbell
dataset made of $43,000$ songs \cite{Bertin-Mahieux2010a}.

\begin{figure}[t]
\begin{center}
\includegraphics[width=.9\columnwidth]{imputation}
\end{center}
\caption{Imputation from different methods.}
\label{fig:imputation}
\end{figure}

\begin{table}[t]
\begin{center}
\begin{tabular}{l|c|c|c|c|c|}
\# beats masked & 1 & 2 & 5 & 10 & 15 \\ \hline \hline
random & $0.166$ & $0.166$ & $0.167$ & $0.167$ & $0.168$  \\
rand. from song & $0.115$ & $0.114$ & $0.114$ & $0.115$ & $0.115$  \\
average & $0.047$ & $0.053$ & $0.062$ & $0.065$ & $0.069$ \\
knn eucl & $0.048$ & $0.049$ & $0.055$ & $0.064$ &  $0.070$ \\
knn kl & $0.049$ & $0.050$ & $0.056$ & $0.066$ &  \\
lin. trans. & $\mathbf{0.044}$ & $\mathbf{0.047}$ & $\mathbf{0.051}$ & $\mathbf{0.053}$ & $\mathbf{0.055}$ \\
codebook & & & & &  \\
SIPLCA & & & & &  \\ \hline
\end{tabular}
\caption{Results based on euclidead distance on $43K$ songs.
Song has to be at least $70$ beats long. 
For ``average'', window is $2$ beats each side of the masked patch.
For ``knn eucl'' and ``knn kl'', window is $10$ beats each side of the masked patch.
For ``lin. trans.'', window is the $2$ previous beats.}
\label{tab:reseucl}
\end{center}
\end{table}

\begin{table}[t]
\begin{center}
\begin{tabular}{l|c|c|c|c|c|}
\# beats masked & 1 & 2 & 5 & 10 & 15 \\ \hline \hline
random & $0.428$ & $0.450$ & $0.461$ & $0.461$ & $0.462$  \\
rand. from song & $0.334$ & $0.351$ & $0.371$ & $0.377$ & $0.380$  \\
average & $0.121$ & $0.154$ & $0.194$ & $0.212$ &  $0.223$ \\
knn eucl & $\mathbf{0.116}$ & $0.136$ & $0.169$ & $0.212$ & $0.233$ \\
knn kl & $\mathbf{0.116}$ & $\mathbf{0.135}$ & $\mathbf{0.167}$ & $0.209$ &  \\
lin. trans. & $0.141$ & $0.157$ & $0.170$ & $\mathbf{0.180}$ & $0.184$ \\
codebook & & & & &  \\
SIPLCA & & & & &  \\ \hline
\end{tabular}
\caption{Results based on symmetric KL divergence on $43K$ songs.
See Table \ref{tab:reseucl} for the exact parameters used.}
\label{tab:reskl}
\end{center}
\end{table}

\begin{figure}[t]
\begin{center}
\includegraphics[width=.9\columnwidth]{lintrans_curves}
\end{center}
\caption{Linear transform results based on the window size.}
\label{fig:lintrans}
\end{figure}


\section{CONCLUSION AND FUTURE WORK}
\label{sec:conclusion}
Push the task as an evaluation for different learning method on such feature.
Way to unify different tasks.

Many algorithms remain to be tried, for instance large scale or deep recurrent
neural network, or using the nonlocal-means algorithm as has been done for
images \cite{Buades2005}. We could boost all those things.

Incorporation of different music-related priors, for instance from chord recognition.

Different kind of masking, e.g. removing only one instrument from the mixture.

Code available at \footnote{Code available at: \url{http://www.columbia.edu/~tb2332/something}}.



% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{tbm_bib}

\end{document}
